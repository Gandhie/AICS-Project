{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ln: failed to create symbolic link 'data/data': File exists\r\n"
     ]
    }
   ],
   "source": [
    "! ln -s /usr/local/courses/lt2318/data/ data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image_data.json      objects.pickle\t       VG_100K\r\n",
      "image_data.json.zip  read_objects_resnet50.py  VG_100K_2\r\n",
      "images2.zip\t     relationships.json\r\n",
      "images.zip\t     relationships.json.zip\r\n"
     ]
    }
   ],
   "source": [
    "! ls data/visual_genome/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from file\n",
    "relationships_from_file = json.load(open('data/visual_genome/relationships.json'))\n",
    "\n",
    "# name/names correction for reading content of nodes in the dataset\n",
    "name_extract = lambda x: x['names'][0].lower() if 'names' in x and len(x['names']) else x['name'].lower() if 'name' in x else '' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of images: 108077\n"
     ]
    }
   ],
   "source": [
    "print('number of images:', len(relationships_from_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_data_from_file = json.load(open('data/visual_genome/image_data.json'))\n",
    "image_id_to_size = {\n",
    "    image_data['image_id']: (image_data['width'], image_data['height'])\n",
    "    for image_data in image_data_from_file\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions for getting descriptions (relations) and bounding boxes (which are normalised by image size in the process).\n",
    "def get_description(relations_data):\n",
    "    triplet = (\n",
    "        name_extract(relation_data['subject']),\n",
    "        relation_data['predicate'].lower(), # synset?\n",
    "        name_extract(relation_data['object']),\n",
    "    )\n",
    "    \n",
    "    return triplet\n",
    "\n",
    "def get_bboxes(relations_data, size):\n",
    "    w0, h0 = size\n",
    "    def normalize(bbox):\n",
    "        x, y, w, h = bbox\n",
    "        return (x/w0, y/h0, w/w0, h/h0, )\n",
    "    \n",
    "    bboxes = (\n",
    "        normalize([relation_data['subject'][d] for d in ['x', 'y', 'w', 'h', ]]),\n",
    "        normalize([relation_data['object'][d] for d in ['x', 'y', 'w', 'h', ]]),\n",
    "    )\n",
    "    \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prepositions = ['over', 'above', 'below', 'under', 'on', 'in', 'right of', 'left of']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading visual data.\n",
    "with open('/usr/local/courses/lt2318/data/visual_genome/objects.pickle', 'rb') as f:\n",
    "    visual_features = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting the visual feature vectors from the previous loaded file based on the object_id of the subject and object in a relation.\n",
    "def get_vis_vecs(relations_data, vis_features):\n",
    "    subid = relations_data['subject']['object_id']\n",
    "    objid = relations_data['object']['object_id']\n",
    "    \n",
    "    try:\n",
    "        target_vf = vis_features[subid]\n",
    "    except KeyError:\n",
    "        target_vf = None\n",
    "    try:\n",
    "        landmark_vf = vis_features[objid]\n",
    "    except KeyError:\n",
    "        landmark_vf = None\n",
    "                \n",
    "    return target_vf, landmark_vf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a dictionary which contains another dictionary for each preposition, which contains two lists - one for targets and one for landmarks.\n",
    "prep_vis_feat = {\n",
    "    p: {}\n",
    "    for p in prepositions\n",
    "}\n",
    "\n",
    "for p in prep_vis_feat:\n",
    "    prep_vis_feat[p]['target'] = []\n",
    "    prep_vis_feat[p]['landmark'] = []\n",
    "\n",
    "# Gets visual feature vectors for relations if their predicate is one of the prepositions in question.\n",
    "# Also checks that there are both target and landmark for each relation, and only keeps the ones that have the full pairs.\n",
    "# This was to deal with the fact that I was initially getting different numbers of targets and landmarks.\n",
    "for img_relations in relationships_from_file:\n",
    "    for relation_data in img_relations['relationships']:\n",
    "        _, p, _ = get_description(relation_data)\n",
    "        \n",
    "        if p in prep_vis_feat:\n",
    "            target_vf, landmark_vf = get_vis_vecs(relation_data, visual_features)\n",
    "            \n",
    "            if target_vf is not None and landmark_vf is not None:\n",
    "                prep_vis_feat[p]['target'].append(target_vf)\n",
    "                prep_vis_feat[p]['landmark'].append(landmark_vf)\n",
    "            else:\n",
    "                pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over 8200 8200\n",
      "above 12583 12583\n",
      "below 3155 3155\n",
      "under 16985 16985\n",
      "on 544509 544509\n",
      "in 183951 183951\n",
      "right of 220 220\n",
      "left of 365 365\n"
     ]
    }
   ],
   "source": [
    "for p in prep_vis_feat:\n",
    "    print(p, len(prep_vis_feat[p][\"target\"]), len(prep_vis_feat[p][\"landmark\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saves the dictionary containing all of the visual feature vectors divided by targets and landmark by preposition to file.\n",
    "with open('pvis_vecs.pkl', 'wb') as sf:\n",
    "    pickle.dump(prep_vis_feat, sf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2048,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(prep_vis_feat[p][\"target\"])\n",
    "prep_vis_feat[p][\"target\"][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over \n",
      " Target variance: 0.4377860509985831 Landmark variance: 0.43583857570843\n",
      "above \n",
      " Target variance: 0.4416074931721027 Landmark variance: 0.4337125514527151\n",
      "below \n",
      " Target variance: 0.4496400695075702 Landmark variance: 0.45063913436680325\n",
      "under \n",
      " Target variance: 0.44614292008054934 Landmark variance: 0.44368264179195344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib64/python3.6/site-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "on \n",
      " Target variance: nan Landmark variance: nan\n",
      "in \n",
      " Target variance: nan Landmark variance: nan\n",
      "right of \n",
      " Target variance: 0.4397678258744153 Landmark variance: 0.4418984124606306\n",
      "left of \n",
      " Target variance: nan Landmark variance: nan\n"
     ]
    }
   ],
   "source": [
    "# Takes all target and landmark vectors from prep_vis_feat. Sets a variable to the first vector of each set\n",
    "# and then adds them together (one addition version and one append version currently). \n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "for p in prep_vis_feat:\n",
    "    targ_vecs = prep_vis_feat[p]['target']\n",
    "    landm_vecs = prep_vis_feat[p]['landmark']\n",
    "    \n",
    "    # creating centroids\n",
    "    targ_centroid = np.array(targ_vecs).sum(0)\n",
    "    landm_centroid = np.array(landm_vecs).sum(0)\n",
    "    \n",
    "    # cosine distances:\n",
    "    targ_cos = np.array([cosine(u, targ_centroid) for u in targ_vecs])\n",
    "    landm_cos = np.array([cosine(u, landm_centroid) for u in landm_vecs])\n",
    "    \n",
    "    # average cosine distances (variance):\n",
    "    targ_var = targ_cos.mean()\n",
    "    landm_var = landm_cos.mean()\n",
    "    \n",
    "    # print or save these numbers\n",
    "    print(p, \"\\n\", \"Target variance:\",  targ_var, \"Landmark variance:\", landm_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
